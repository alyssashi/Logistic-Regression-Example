---
title: "Logistic Regression"
author: "Alyssa Shi"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE, 
                      message=FALSE)
```

## Load packages & data

```{r load-packages, message=FALSE}
library(tidyverse)
library(knitr)
library(broom)
library(pROC)
library(plotROC)
```

```{r load-data}
spotify <- read.csv("Data/spotify.txt")
```

### Data Wrangling and EDA

Refactoring variables:

```{r}
spotify <- spotify %>%
  mutate(key = case_when(
    key == 2 ~ "D",
    key == 3 ~ "D#",
    key == 0 | key == 1 | key > 3 ~ "Other"),
    key = as.factor(key),
    like = as.factor(like))

levels(spotify$like) <- c("No", "Yes")
```

EDA:

```{r}
ggplot(data = spotify, mapping = aes(x = key, fill = like)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", x = 'Key', fill = "Songs Liked")
spotify %>%
  group_by(key, like) %>%
  summarise(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  filter(like == "Yes")
```

Songs in the key D have the highest proportion of likes (59.24%), followed by Other (50.34%) and D# (31.75%).

### Model Fitting

Fit a logistic regression model with `like` as the response variable and the following as predictors: `acousticness`, `danceability`, `duration_ms`, `instrumentalness`, `loudness`, `speechiness`, and `valence`:

```{r}
logit_red <- glm(like ~ acousticness + danceability + duration_ms + 
                  instrumentalness + loudness + speechiness + valence,
                  spotify, family = binomial)
tidy(logit_red, conf.int = T) %>%
  kable(format = "markdown", digits = 3)
```

### Drop in Deviance

Add the variable `key` to the model created above and use a drop-in deviance test to see if this variable should be included. 

The null hypothesis is that all the new coefficients (levels D, D#, or Other) of `key` are equal to zero. 

The alternative hypothesis is that at least one of the new coefficients (levels D, D#, or Other) of `key` is not equal to zero.

```{r}
logit_full <- glm(like ~ acousticness + danceability + duration_ms + 
                  instrumentalness + loudness + speechiness + valence + key,
                  spotify, family = binomial)
anova(logit_red, logit_full, test = "Chisq")
```

Because the p-value for the drop-in deviance test is 0.001258 which is less than .05, our model would benefit from adding `key` as a predictor variable. We can double check this conclusion using AIC.

```{r}
logit_red$aic
logit_full$aic
```

The AIC for the model from Exercise 3 with the addition of `key` as a predictor variable is 2525.16 compared to the AIC of the model without the addition of `key` at 2534.517. The model with `key` is more ideal because a lower AIC indicates a better fitted model.

### Interpretation

```{r}
tidy(logit_full, conf.int = T) %>%
  kable(format = "markdown", digits = 3)
```

The predicted odds that a user likes a song in the key D# are exp{-1.073)} = `r round (exp(-1.073),3)` times the odds of a like for songs in the key D (the baseline), holding all else constant.

### Calculating Predicted Probabilities and Residuals

```{r}
logit_aug <- augment(logit_full, 
                          type.predict = "response",
                          type.residuals = "response")
```

### Checking Assumptions (Linearity)

Create a binned plot of the residuals versus the predicted probabilities:

```{r}
arm::binnedplot(x = logit_aug$.fitted, 
                y = logit_aug$.resid, 
                xlab = "Predicted Probabilities", 
                main = "Binned Residual vs. Predicted Values",  
                col.int = FALSE)
```

We chose the quantitative variable `duration_ms` and used a binned residuals plot to examine the residuals versus this predictor variable.

```{r}
arm::binnedplot(x = logit_aug$duration_ms, 
                y = logit_aug$.resid, 
                col.int = FALSE, 
                xlab = "Duration of Track (milliseconds)",
                main = "Binned Residual vs. Duration of Track (milliseconds)")
```

We chose the categorical variable `key` and found the mean value of the residuals for each of the three levels.

```{r}
logit_aug %>%
  group_by(key) %>%
  summarise(mean_resid = mean(.resid))
```

Based on the plots from exercises 6-7, it seems as though the linearity assumption is satisfied. This is because there is no apparent shape in the plots of the predicted values versus the residuals, nor do the observations follow any sorts of linear lines. Additionally, the mean residuals in exercise 8 are all very close to 0, indicating that the linearity assumption is satisfied for `key`.  

### ROC and AUC

ROC curve and calculating AUC:

```{r}
(roc_curve <- ggplot(logit_aug, 
                     aes(d = as.numeric(like) - 1, 
                         m = .fitted)) +
  geom_roc(n.cuts = 5, labelround = 3) + 
  geom_abline(intercept = 0) + 
  labs(title = "ROC Curve of Like",
       x = "False Postive Rate", 
       y = "True Positive Rate") )
```

```{r}
calc_auc(roc_curve)$AUC
```

As seen above, the area under the curve for the ROC curve is approximately 0.714.

### Interpretation

Based on the ROC curve and AUC from the previous exercise, it seems as though this model does about an average job of differentiating between songs the user likes and doesn't like. An AUC score closer to 1 is typically a better fit (whereas 0.5 means a very bad fit), and the AUC score of this particular curve was 0.714 which was almost exactly in the middle of the spectrum.

### Choosing a Threshold

The threshold value we chose was 0.5. This is because, as we examined the ROC curve, we thought a threshold value of around 0.5 would be the best compromise for a high true positive rate and low false positive rate. After a threshold value of 0.5, the slope of the ROC curve seems to decrease, meaning a lower increase in the true positive rate compared to a greater increase in the false positive rate. 

Keeping in mind Spotify's goal of recommending songs a user has a high liklihood of liking, we decided that a threshold of 0.5 would recommend songs that a user has a pretty high probability of liking, without annoying the user by recommending a bunch of songs they don't like.

### Creating Confusion Matrix

```{r thresh}
threshold <- 0.5
logit_aug %>%
  mutate(like_predict = if_else(.fitted > threshold, "Yes", "No")) %>%
  group_by(like, like_predict) %>%
  summarise(n = n()) %>%
  kable(format="markdown")
```

### Misclassification Rates

The proportion of true positives is: $$668/(668 + 352) \approx 0.655$$

The proportion of false positives is: $$1 - 687/(687 + 310) \approx 0.311 $$

The misclassification rate is: $$(310 + 352)/(687 + 310 + 352 + 668) \approx 0.328$$